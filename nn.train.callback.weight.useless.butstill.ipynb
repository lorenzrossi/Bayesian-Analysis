{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfk = tf.keras\n",
    "tf.keras.backend.set_floatx(\"float64\")\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global = pd.read_csv('splitted/Total.csv')\n",
    "df_global.drop(columns=[col for col in df_global.columns if col.startswith('time')], inplace=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_generation biomass</th>\n",
       "      <th>y_generation nuclear</th>\n",
       "      <th>y_generation other</th>\n",
       "      <th>y_generation other renewable</th>\n",
       "      <th>y_generation solar</th>\n",
       "      <th>y_generation waste</th>\n",
       "      <th>y_price actual</th>\n",
       "      <th>y_generation fossil</th>\n",
       "      <th>y_generation hydro</th>\n",
       "      <th>y_generation wind</th>\n",
       "      <th>...</th>\n",
       "      <th>x_temp_min_Bilbao</th>\n",
       "      <th>x_temp_max_Bilbao</th>\n",
       "      <th>x_pressure_Bilbao</th>\n",
       "      <th>x_humidity_Bilbao</th>\n",
       "      <th>x_wind_speed_Bilbao</th>\n",
       "      <th>x_wind_deg_Bilbao</th>\n",
       "      <th>x_rain_1h_Bilbao</th>\n",
       "      <th>x_rain_3h_Bilbao</th>\n",
       "      <th>x_snow_3h_Bilbao</th>\n",
       "      <th>x_clouds_all_Bilbao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>447.0</td>\n",
       "      <td>7096.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>65.41</td>\n",
       "      <td>10156.0</td>\n",
       "      <td>3813.0</td>\n",
       "      <td>6378.0</td>\n",
       "      <td>...</td>\n",
       "      <td>269.657312</td>\n",
       "      <td>269.657312</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>449.0</td>\n",
       "      <td>7096.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>64.92</td>\n",
       "      <td>10437.0</td>\n",
       "      <td>3587.0</td>\n",
       "      <td>5890.0</td>\n",
       "      <td>...</td>\n",
       "      <td>269.763500</td>\n",
       "      <td>269.763500</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>448.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>64.48</td>\n",
       "      <td>9918.0</td>\n",
       "      <td>3508.0</td>\n",
       "      <td>5461.0</td>\n",
       "      <td>...</td>\n",
       "      <td>269.251688</td>\n",
       "      <td>269.251688</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>438.0</td>\n",
       "      <td>7098.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>59.32</td>\n",
       "      <td>8859.0</td>\n",
       "      <td>3231.0</td>\n",
       "      <td>5238.0</td>\n",
       "      <td>...</td>\n",
       "      <td>269.203344</td>\n",
       "      <td>269.203344</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>428.0</td>\n",
       "      <td>7097.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>56.04</td>\n",
       "      <td>8313.0</td>\n",
       "      <td>3499.0</td>\n",
       "      <td>4935.0</td>\n",
       "      <td>...</td>\n",
       "      <td>269.485500</td>\n",
       "      <td>269.485500</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_generation biomass  y_generation nuclear  y_generation other  \\\n",
       "0                 447.0                7096.0                43.0   \n",
       "1                 449.0                7096.0                43.0   \n",
       "2                 448.0                7099.0                43.0   \n",
       "3                 438.0                7098.0                43.0   \n",
       "4                 428.0                7097.0                43.0   \n",
       "\n",
       "   y_generation other renewable  y_generation solar  y_generation waste  \\\n",
       "0                          73.0                49.0               196.0   \n",
       "1                          71.0                50.0               195.0   \n",
       "2                          73.0                50.0               196.0   \n",
       "3                          75.0                50.0               191.0   \n",
       "4                          74.0                42.0               189.0   \n",
       "\n",
       "   y_price actual  y_generation fossil  y_generation hydro  y_generation wind  \\\n",
       "0           65.41              10156.0              3813.0             6378.0   \n",
       "1           64.92              10437.0              3587.0             5890.0   \n",
       "2           64.48               9918.0              3508.0             5461.0   \n",
       "3           59.32               8859.0              3231.0             5238.0   \n",
       "4           56.04               8313.0              3499.0             4935.0   \n",
       "\n",
       "   ...  x_temp_min_Bilbao  x_temp_max_Bilbao  x_pressure_Bilbao  \\\n",
       "0  ...         269.657312         269.657312             1036.0   \n",
       "1  ...         269.763500         269.763500             1035.0   \n",
       "2  ...         269.251688         269.251688             1036.0   \n",
       "3  ...         269.203344         269.203344             1035.0   \n",
       "4  ...         269.485500         269.485500             1035.0   \n",
       "\n",
       "   x_humidity_Bilbao  x_wind_speed_Bilbao  x_wind_deg_Bilbao  \\\n",
       "0                 97                  0.0                226   \n",
       "1                 97                  0.0                229   \n",
       "2                 97                  1.0                224   \n",
       "3                 97                  1.0                225   \n",
       "4                 97                  1.0                221   \n",
       "\n",
       "   x_rain_1h_Bilbao  x_rain_3h_Bilbao  x_snow_3h_Bilbao  x_clouds_all_Bilbao  \n",
       "0               0.0               0.0               0.0                    0  \n",
       "1               0.0               0.0               0.0                    0  \n",
       "2               0.0               0.0               0.0                    0  \n",
       "3               0.0               0.0               0.0                    0  \n",
       "4               0.0               0.0               0.0                    0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_global.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nn = MinMaxScaler().fit_transform(df_global)\n",
    "dataset = pd.DataFrame(nn, columns=df_global.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inputs = [col for col in df_global.columns if col.startswith('x_')]\n",
    "outputs = [col for col in df_global.columns if col.startswith('y_')]\n",
    "\n",
    "x = dataset[inputs]\n",
    "y = dataset[outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define helper functions.\n",
    "detector = IsolationForest(n_estimators=1000, random_state=SEED)\n",
    "neg_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define some hyperparameters.\n",
    "n_epochs, n_batches, n_samples = 20, 16, dataset.shape[0]\n",
    "buffer_size, batch_size = n_samples, np.floor(n_samples/n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define training and test data sizes.\n",
    "n_train, n_test = int(0.70*n_samples), int(0.30*n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define dataset instance.\n",
    "data = tf.data.Dataset.from_tensor_slices((x.values, y.values))\n",
    "data = data.shuffle(n_samples, reshuffle_each_iteration=True, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define train and test data instances.\n",
    "data_train = data.take(n_train).batch(batch_size).repeat(n_epochs)\n",
    "data_test = data.skip(n_train).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define prior for regularization.\n",
    "\n",
    "gauss_prior = tfd.Normal(\n",
    "    loc = tf.zeros( len(outputs), dtype=tf.float64 ),\n",
    "    scale = 1.0, #tf.ones( len(outputs), dtype=tf.float64 ),\n",
    "    name = \"p_normal\",\n",
    "    )\n",
    "\n",
    "prior = tfd.Independent(\n",
    "    gauss_prior,\n",
    "    reinterpreted_batch_ndims = 1,\n",
    "    name = \"prior\",\n",
    "    )\n",
    "\n",
    "regularizer = tfp.layers.KLDivergenceRegularizer(prior, weight=1/n_batches, ) # Kullbackâ€“Leibler divergence (also called relative entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_size(n) : return sum(range(n+2))-1 # N means + N(N+1)/2 co-variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define model instance.\n",
    "model = tfk.Sequential([\n",
    "\n",
    "    tfk.layers.InputLayer(\n",
    "        input_shape=(len(inputs),),\n",
    "        name=\"input\"\n",
    "        ),\n",
    "\n",
    "    # dense for inputs\n",
    "    tfk.layers.Dense(\n",
    "        n_batches,\n",
    "        activation=\"relu\",\n",
    "        name=\"dense_input\"\n",
    "        ),\n",
    "\n",
    "    # # a new dense for input\n",
    "    # tfp.layers.DenseFlipout(\n",
    "    #     10,\n",
    "    #     activation=\"relu\",\n",
    "    #     name=\"dense_1\"\n",
    "    # )\n",
    "\n",
    "    # dense for weights\n",
    "    tfk.layers.Dense(\n",
    "        params_size(len(outputs)), # uncertainty in the parameters weights\n",
    "        activation=None,\n",
    "        name=\"distribution_weights\"\n",
    "        ),\n",
    "\n",
    "    # (declaration of the) posterior probability distribution structure\n",
    "    tfp.layers.MultivariateNormalTriL(\n",
    "        len(outputs),\n",
    "        # activity_regularizer acts as prior for the output layer\n",
    "        activity_regularizer=regularizer, \n",
    "        name=\"output\"),\n",
    "        \n",
    "    ], name=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compile model.\n",
    "model.compile(optimizer=\"adam\", loss=neg_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\" ... i'm gonna pickle the model: \")\n",
    "        for l in ['dense_input', 'distribution_weights', 'output']:\n",
    "            tmp = model.get_layer(l)\n",
    "            try:\n",
    "                ump = list()\n",
    "                for i in range(1000):\n",
    "                    try:\n",
    "                        ump.append(tmp.get_input_at(i))\n",
    "                    except:\n",
    "                        break\n",
    "                model_history[epoch,l,'input'] = ump\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                model_history[epoch,l,'weight'] = tmp.weights.copy()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                ump = list()\n",
    "                for i in range(1000):\n",
    "                    try:\n",
    "                        ump.append(tmp.get_output_at(i))\n",
    "                    except:\n",
    "                        break\n",
    "                model_history[epoch,l,'output'] = ump\n",
    "            except:\n",
    "                pass\n",
    "            print(f'{epoch}th {l} state pickled!')\n",
    "        # keys = list(logs.keys())\n",
    "        # print('-+'*20)\n",
    "        # print(f\"End epoch {epoch} of training; got log keys: {keys}\")\n",
    "        # print('-+'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "239/240 [============================>.] - ETA: 0s - loss: 1.8502-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 0 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 96s 379ms/step - loss: 1.8460 - val_loss: -2.9443\n",
      "Epoch 2/20\n",
      "239/240 [============================>.] - ETA: 0s - loss: -3.5769-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 1 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 90s 376ms/step - loss: -3.5771 - val_loss: -3.9101\n",
      "Epoch 3/20\n",
      "239/240 [============================>.] - ETA: 0s - loss: -4.0596-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 2 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 82s 343ms/step - loss: -4.0595 - val_loss: -4.1262\n",
      "Epoch 4/20\n",
      "239/240 [============================>.] - ETA: 0s - loss: -4.2303-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 3 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 89s 372ms/step - loss: -4.2303 - val_loss: -4.3054\n",
      "Epoch 5/20\n",
      "239/240 [============================>.] - ETA: 0s - loss: -4.3531-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 4 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 83s 345ms/step - loss: -4.3532 - val_loss: -4.4048\n",
      "Epoch 6/20\n",
      "240/240 [==============================] - ETA: 0s - loss: -4.4646-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 5 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 77s 319ms/step - loss: -4.4646 - val_loss: -4.5262\n",
      "Epoch 7/20\n",
      "240/240 [==============================] - ETA: 0s - loss: -4.5474-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 6 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 80s 332ms/step - loss: -4.5474 - val_loss: -4.5928\n",
      "Epoch 8/20\n",
      "239/240 [============================>.] - ETA: 0s - loss: -4.6053-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 7 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 76s 316ms/step - loss: -4.6053 - val_loss: -4.6409\n",
      "Epoch 9/20\n",
      "239/240 [============================>.] - ETA: 0s - loss: -4.6492-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 8 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 82s 343ms/step - loss: -4.6491 - val_loss: -4.6500\n",
      "Epoch 10/20\n",
      "240/240 [==============================] - ETA: 0s - loss: -4.6838-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 9 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 80s 335ms/step - loss: -4.6838 - val_loss: -4.6997\n",
      "Epoch 11/20\n",
      "239/240 [============================>.] - ETA: 0s - loss: -4.7118-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 10 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 86s 358ms/step - loss: -4.7118 - val_loss: -4.7257\n",
      "Epoch 12/20\n",
      "239/240 [============================>.] - ETA: 0s - loss: -4.7270-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 11 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 74s 310ms/step - loss: -4.7273 - val_loss: -4.7583\n",
      "Epoch 13/20\n",
      "239/240 [============================>.] - ETA: 0s - loss: -4.7723-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 12 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 73s 305ms/step - loss: -4.7723 - val_loss: -4.7912\n",
      "Epoch 14/20\n",
      "239/240 [============================>.] - ETA: 0s - loss: -4.8141-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 13 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 74s 309ms/step - loss: -4.8142 - val_loss: -4.8443\n",
      "Epoch 15/20\n",
      "239/240 [============================>.] - ETA: 0s - loss: -4.8462-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 14 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 72s 300ms/step - loss: -4.8460 - val_loss: -4.8546\n",
      "Epoch 16/20\n",
      "239/240 [============================>.] - ETA: 0s - loss: -4.8660-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 15 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 77s 321ms/step - loss: -4.8659 - val_loss: -4.8959\n",
      "Epoch 17/20\n",
      "239/240 [============================>.] - ETA: 0s - loss: -4.8900-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 16 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 73s 304ms/step - loss: -4.8898 - val_loss: -4.9380\n",
      "Epoch 18/20\n",
      "239/240 [============================>.] - ETA: 0s - loss: -4.9151-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 17 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 71s 295ms/step - loss: -4.9153 - val_loss: -4.9552\n",
      "Epoch 19/20\n",
      "239/240 [============================>.] - ETA: 0s - loss: -4.9404-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 18 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 68s 283ms/step - loss: -4.9406 - val_loss: -4.9598\n",
      "Epoch 20/20\n",
      "238/240 [============================>.] - ETA: 0s - loss: -4.9648-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "End epoch 19 of training; got log keys: ['loss', 'val_loss']\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
      "240/240 [==============================] - 68s 283ms/step - loss: -4.9649 - val_loss: -5.0103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x215c2bd7a30>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_train, epochs=n_epochs, validation_data=data_test, verbose=True, callbacks=[CustomCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_input (Dense)         (None, 16)                896       \n",
      "                                                                 \n",
      " distribution_weights (Dense  (None, 65)               1105      \n",
      " )                                                               \n",
      "                                                                 \n",
      " output (MultivariateNormalT  ((None, 10),             0         \n",
      " riL)                         (None, 10))                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,001\n",
      "Trainable params: 2,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Describe model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump({k:model_history[k] for k in model_history if 'weight' in k}, open('model.input.weght.output.history.20.epoch.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
